[INFO|utils|numexpr.utils|L160] - 2024-06-06 23:14:18  ->  NumExpr defaulting to 4 threads.
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:14:27  ->  Using proactor: IocpProactor
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:14:27  ->  Using proactor: IocpProactor
[DEBUG|_config|httpx|L80] - 2024-06-06 23:14:27  ->  load_ssl_context verify=True cert=None trust_env=True http2=False
[DEBUG|_config|httpx|L146] - 2024-06-06 23:14:27  ->  load_verify_locations cafile='C:\\Users\\rm\\anaconda3\\envs\\DataEngineeringEnv\\Library\\ssl\\cacert.pem'
[INFO|storage|WEB_SCRAPER|L81] - 2024-06-06 23:14:29  ->  Connecting to database mysql
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:14:29  ->  caching sha2: succeeded by fast path.
[INFO|storage|WEB_SCRAPER|L95] - 2024-06-06 23:14:29  ->  Successfully Connected to database!
[INFO|beautifulcrawler|WEB_SCRAPER|L423] - 2024-06-06 23:14:29  ->  Worker(s) starting work...
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:14:29  ->  Worker-1 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:14:29  ->  Worker-2 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:14:29  ->  Worker-3 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:14:32  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:14:32  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:14:32  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:14:32  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:14:32  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:14:32  ->  Request: GET https://books.toscrape.com
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:14:32  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:14:32  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:14:32  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:14:37  ->  connect_tcp.failed exception=ConnectTimeout(TimeoutError())
[ERROR|beautifulcrawler|WEB_SCRAPER|L95] - 2024-06-06 23:14:37  ->  Unable to get page!!!
[ERROR|beautifulcrawler|WEB_SCRAPER|L96] - 2024-06-06 23:14:37  ->  Exception: ConnectTimeout: 
[INFO|beautifulcrawler|WEB_SCRAPER|L465] - 2024-06-06 23:14:37  ->  Worker-3 completed task in time:  7.61s
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:14:37  ->  connect_tcp.failed exception=ConnectTimeout(TimeoutError())
[ERROR|beautifulcrawler|WEB_SCRAPER|L95] - 2024-06-06 23:14:37  ->  Unable to get page!!!
[ERROR|beautifulcrawler|WEB_SCRAPER|L96] - 2024-06-06 23:14:37  ->  Exception: ConnectTimeout: 
[INFO|beautifulcrawler|WEB_SCRAPER|L465] - 2024-06-06 23:14:37  ->  Worker-2 completed task in time:  7.62s
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:14:37  ->  connect_tcp.failed exception=ConnectTimeout(TimeoutError())
[ERROR|beautifulcrawler|WEB_SCRAPER|L95] - 2024-06-06 23:14:37  ->  Unable to get page!!!
[ERROR|beautifulcrawler|WEB_SCRAPER|L96] - 2024-06-06 23:14:37  ->  Exception: ConnectTimeout: 
[INFO|beautifulcrawler|WEB_SCRAPER|L465] - 2024-06-06 23:14:37  ->  Worker-1 completed task in time:  7.63s
[INFO|beautifulcrawler|WEB_SCRAPER|L432] - 2024-06-06 23:14:37  ->  Worker(s) finished all work in time: 7.64s
[INFO|utils|numexpr.utils|L160] - 2024-06-06 23:15:38  ->  NumExpr defaulting to 4 threads.
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:15:40  ->  Using proactor: IocpProactor
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:15:40  ->  Using proactor: IocpProactor
[DEBUG|_config|httpx|L80] - 2024-06-06 23:15:40  ->  load_ssl_context verify=True cert=None trust_env=True http2=False
[DEBUG|_config|httpx|L146] - 2024-06-06 23:15:40  ->  load_verify_locations cafile='C:\\Users\\rm\\anaconda3\\envs\\DataEngineeringEnv\\Library\\ssl\\cacert.pem'
[INFO|storage|WEB_SCRAPER|L81] - 2024-06-06 23:15:41  ->  Connecting to database mysql
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:15:41  ->  caching sha2: succeeded by fast path.
[INFO|storage|WEB_SCRAPER|L95] - 2024-06-06 23:15:41  ->  Successfully Connected to database!
[INFO|beautifulcrawler|WEB_SCRAPER|L423] - 2024-06-06 23:15:41  ->  Worker(s) starting work...
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:15:41  ->  Worker-1 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:15:41  ->  Worker-2 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:15:41  ->  Worker-3 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:44  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:44  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:44  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:44  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:44  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:44  ->  Request: GET https://books.toscrape.com
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F4467640>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x00000224F43EB0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F442DBB0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x00000224F43EB0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F44A34F0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:44  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x00000224F43EB0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:45  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F44A36D0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:45  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F44674C0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:45  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F449DC40>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:45  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:45  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:45  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:45  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:45  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:45  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:45  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:48  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:48  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:48  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:48  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:48  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:48  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:19 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:48  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:48  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:19 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:48  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:48  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:19 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:48  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:48  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:48  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:15:48  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:15:48  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:48  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing save_to_db on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:15:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:15:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:15:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:15:49  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:49  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:15:49  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:15:49  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing save_to_db on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:15:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:15:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:15:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:15:49  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:49  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:49  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:49  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:15:49  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:15:49  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:49  ->  Executing save_to_db on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:15:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:15:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:15:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:15:49  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:51  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:51  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:51  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:51  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:51  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:52  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:52  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:15:52  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:15:52  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing save_to_db on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:15:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:15:52  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L336] - 2024-06-06 23:15:52  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:15:52  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:15:52  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing save_to_db on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:15:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:15:52  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L336] - 2024-06-06 23:15:52  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:15:52  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:15:52  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:52  ->  Executing save_to_db on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:15:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:15:52  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L336] - 2024-06-06 23:15:52  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:55  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:55  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:15:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:15:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:55  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:55  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:15:55  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:15:55  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:55  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:55  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:55  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:55  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:55  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:55  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:55  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:55  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:55  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:55  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:15:55  ->  Executing save_to_db on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:15:55  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:15:55  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:15:55  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:15:55  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:15:55  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:16:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:15:55  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:15:55  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_body.failed exception=CancelledError()
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  receive_response_body.failed exception=CancelledError()
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  response_closed.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:55  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:55  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:55  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:55  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:55  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:15:55  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:15:55  ->  response_closed.complete
[INFO|utils|numexpr.utils|L160] - 2024-06-06 23:17:49  ->  NumExpr defaulting to 4 threads.
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:17:51  ->  Using proactor: IocpProactor
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:17:51  ->  Using proactor: IocpProactor
[DEBUG|_config|httpx|L80] - 2024-06-06 23:17:51  ->  load_ssl_context verify=True cert=None trust_env=True http2=False
[DEBUG|_config|httpx|L146] - 2024-06-06 23:17:51  ->  load_verify_locations cafile='C:\\Users\\rm\\anaconda3\\envs\\DataEngineeringEnv\\Library\\ssl\\cacert.pem'
[INFO|storage|WEB_SCRAPER|L81] - 2024-06-06 23:17:51  ->  Connecting to database mysql
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:17:52  ->  caching sha2: succeeded by fast path.
[INFO|storage|WEB_SCRAPER|L95] - 2024-06-06 23:17:52  ->  Successfully Connected to database!
[INFO|beautifulcrawler|WEB_SCRAPER|L423] - 2024-06-06 23:17:52  ->  Worker(s) starting work...
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:17:52  ->  Worker-1 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:17:52  ->  Worker-2 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L461] - 2024-06-06 23:17:52  ->  Worker-3 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:17:54  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:17:54  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:17:54  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:17:54  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:17:54  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:17:54  ->  Request: GET https://books.toscrape.com
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F08554F0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x00000224F079A0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F07DDBB0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x00000224F079A0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F0816640>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:54  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x00000224F079A0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:55  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F084DC40>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:55  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F08164C0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:17:55  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000224F08556D0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:17:55  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:17:55  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:17:55  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:17:55  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:17:55  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:17:55  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:55  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:17:58  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:17:58  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:17:58  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:17:58  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:17:58  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:17:58  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:29 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:17:58  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:17:58  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:29 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:17:58  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:17:58  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:29 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:17:58  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:17:58  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:17:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:17:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:17:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:17:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:17:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:17:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:17:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:17:59  ->  Executing save_to_db on dataset
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:17:59  ->  caching sha2: succeeded by fast path.
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:17:59  ->  caching sha2: succeeded by fast path.
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:17:59  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:17:59  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:17:59  ->  Saving to file instead
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:59  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:17:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:17:59  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:17:59  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:17:59  ->  Moving on!
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:17:59  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:17:59  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:17:59  ->  Saving to file instead
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:59  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:17:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:17:59  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:17:59  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:17:59  ->  Moving on!
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:17:59  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:17:59  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:17:59  ->  Saving to file instead
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:17:59  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:17:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:17:59  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:17:59  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:17:59  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:18:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:18:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:18:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:18:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:18:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:18:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:18:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:18:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:18:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:18:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:18:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:18:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:18:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:18:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:18:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:18:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:02  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:18:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:18:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:02  ->  Executing save_to_db on dataset
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:18:02  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:18:02  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:18:02  ->  Saving to file instead
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:18:02  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:18:02  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:18:02  ->  Saving to file instead
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:18:02  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:18:02  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:18:02  ->  Saving to file instead
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:02  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:18:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:18:02  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L336] - 2024-06-06 23:18:03  ->  going to next page
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:03  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:18:03  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:18:03  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L336] - 2024-06-06 23:18:03  ->  going to next page
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:03  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:18:03  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:18:03  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L336] - 2024-06-06 23:18:03  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:18:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:18:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:18:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:18:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L86] - 2024-06-06 23:18:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L71] - 2024-06-06 23:18:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:18:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:18:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:18:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:18:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:18:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:18:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L76] - 2024-06-06 23:18:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:18:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:18:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:18:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:18:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:18:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L265] - 2024-06-06 23:18:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L377] - 2024-06-06 23:18:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L394] - 2024-06-06 23:18:06  ->  Executing save_to_db on dataset
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:18:06  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:18:06  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:18:06  ->  Saving to file instead
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:18:06  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:18:06  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:18:06  ->  Saving to file instead
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:18:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:18:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:18:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:18:06  ->  Moving on!
[ERROR|storage|WEB_SCRAPER|L176] - 2024-06-06 23:18:06  ->  Unable to get row_id!!!
[ERROR|storage|WEB_SCRAPER|L177] - 2024-06-06 23:18:06  ->  Exception: ProgrammingError: (1146, "Table 'scraping.bookscrape' doesn't exist")
[INFO|storage|WEB_SCRAPER|L180] - 2024-06-06 23:18:06  ->  Saving to file instead
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:18:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:18:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:18:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:18:06  ->  Moving on!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L396] - 2024-06-06 23:18:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L406] - 2024-06-06 23:18:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L325] - 2024-06-06 23:18:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L248] - 2024-06-06 23:18:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:18:06  ->  Moving on!
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:18:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:18:08  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:18:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:18:08  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:18:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:18:08  ->  close.complete
[INFO|utils|numexpr.utils|L160] - 2024-06-06 23:24:25  ->  NumExpr defaulting to 4 threads.
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:24:26  ->  Using proactor: IocpProactor
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:24:26  ->  Using proactor: IocpProactor
[DEBUG|_config|httpx|L80] - 2024-06-06 23:24:26  ->  load_ssl_context verify=True cert=None trust_env=True http2=False
[DEBUG|_config|httpx|L146] - 2024-06-06 23:24:26  ->  load_verify_locations cafile='C:\\Users\\rm\\anaconda3\\envs\\DataEngineeringEnv\\Library\\ssl\\cacert.pem'
[INFO|storage|WEB_SCRAPER|L81] - 2024-06-06 23:24:27  ->  Connecting to database mysql
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:24:27  ->  caching sha2: succeeded by fast path.
[INFO|storage|WEB_SCRAPER|L95] - 2024-06-06 23:24:27  ->  Successfully Connected to database!
[INFO|beautifulcrawler|WEB_SCRAPER|L424] - 2024-06-06 23:24:32  ->  Worker(s) starting work...
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:24:32  ->  Worker-1 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:24:32  ->  Worker-2 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:24:32  ->  Worker-3 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:34  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:34  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:34  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:34  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:34  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:34  ->  Request: GET https://books.toscrape.com
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:34  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:34  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:34  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4C8A67C0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4C8E7400>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4C86D5B0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4C8E7730>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4C8DFC70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:35  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4C8A6640>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:35  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:35  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:35  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:35  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:35  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:35  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:38  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:38  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:38  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:39  ->  Executing save_to_db on dataset
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:24:39  ->  caching sha2: succeeded by fast path.
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:24:39  ->  caching sha2: succeeded by fast path.
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:42  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:42  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:42  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:42  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:24:42  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:24:42  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:43  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:43  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:24:43  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:24:43  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:43  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:43  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:24:43  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:24:43  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:45  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:45  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:45  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:45  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:45  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:45  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:45  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB33A00>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CBC6100>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCD1550>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB36880>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB2DD30>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:46  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB33FD0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:17 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:17 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:17 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:46  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:46  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:46  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:46  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:46  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:46  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:46  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:46  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:46  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:46  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:46  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:46  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:46  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:47  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:47  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:47  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:47  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:47  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:47  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:47  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:47  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:47  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:47  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:47  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:47  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:47  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:47  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:50  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:50  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:50  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:24:50  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:50  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:50  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:50  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:24:50  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:50  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:50  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:50  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:24:50  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:53  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:53  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:53  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:53  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:53  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:53  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCDF250>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCDD640>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CD83E50>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE38F40>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CD8E670>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:24:53  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CD83D60>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:53  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:25 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:54  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:54  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:25 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:54  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:54  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:25 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:54  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:54  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:54  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:54  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:54  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:54  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:54  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:54  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:54  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:54  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:54  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:24:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:56  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:56  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:24:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:24:56  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:56  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:56  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:24:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:24:56  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:24:56  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:56  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:24:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:24:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:24:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:24:56  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:24:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:24:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3bd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3bd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3bd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:24:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:24:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:24:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:24:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:59  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:59  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:59  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:24:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:24:59  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:24:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:00  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:00  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:00  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:00  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:00  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:00  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:00  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:00  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:00  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:05  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:05  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:05  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:05  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:05  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:05  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:05  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:05  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:05  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:05  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:05  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:05  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:07  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:07  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:07  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:07  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:07  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:07  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:07  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:07  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:07  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CECCC40>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CECC790>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CFEDBB0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CED4F40>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:39 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7157"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:08  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:08  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:08  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:08  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:08  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:08  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:08  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:08  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:08  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:08  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:08  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:08  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:08  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:08  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:08  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB24FD0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:08  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CC7DC70>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:08  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7157"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7157"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:09  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:10  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:10  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:10  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:10  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:10  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:10  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8ff"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:12  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:12  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:44 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8ff"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:44 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8ff"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:13  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:16  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:16  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:16  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:16  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:16  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:17  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:17  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:17  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:17  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:17  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:17  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:17  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:17  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:19  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:19  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:19  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:19  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:19  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:19  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D02B880>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D03A490>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE32040>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:19  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:20  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB364F0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:20  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CD4BB20>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:20  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D093940>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:51 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c7dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:20  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:20  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:51 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c7dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:20  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:20  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:51 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c7dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:20  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:20  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:21  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:21  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:21  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:21  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:21  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:24  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:24  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:24  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:24  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:24  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:24  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:24  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:24  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:24  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:24  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:24  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:24  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:24  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:24  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:24  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D1ADF70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CD589D0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D1B5640>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:27  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:28  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D10B520>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:28  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D1B5AC0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:28  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D1B5730>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8c0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8c0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:25:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8c0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:29  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:29  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:29  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:29  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:29  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:29  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:29  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:29  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:29  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:29  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:32  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:32  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:32  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:32  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:32  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:32  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:32  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:32  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:32  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:32  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:32  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:32  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:32  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:32  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:32  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:34  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:34  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:34  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:34  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:34  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:34  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:34  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:34  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:34  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:35  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:35  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:35  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:35  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CD4BC10>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D47CA00>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D317D30>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D321730>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D47CDF0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:35  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D47CA90>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ab40"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:35  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:35  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ab40"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:35  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:35  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:07 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ab40"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:36  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:36  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:36  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:39  ->  15 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:39  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:39  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:39  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:39  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:39  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:39  ->  15 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:39  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:39  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:39  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:39  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:39  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:39  ->  15 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:39  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:39  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:39  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:39  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:39  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:41  ->  Getting page: https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:41  ->  Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:41  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:41  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:41  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:41  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:41  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:41  ->  Getting page: https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:41  ->  Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:41  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5A9670>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3CD2E0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D63F730>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D637D60>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D637850>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:42  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D637B80>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:13 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bb02"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:42  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:42  ->  Response: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:13 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bb02"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:42  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:42  ->  Response: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:13 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bb02"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:42  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:42  ->  Response: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:42  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:42  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:42  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:42  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:42  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:42  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:42  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:42  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:42  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:42  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:43  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:46  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:46  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:46  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:46  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:46  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:46  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:46  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:46  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:46  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:46  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:46  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:46  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:46  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:46  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:46  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:46  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:46  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:46  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5BD790>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5B33D0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5C27C0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3CDC40>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D47CDC0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:25:49  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D7A4F70>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:49  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-90a8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-90a8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-90a8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:50  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:52  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:52  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:52  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:52  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:52  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:52  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:52  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:52  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:52  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:52  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:25:52  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:25:52  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:25:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:25:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:55  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:55  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:55  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:55  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:25:55  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:25:55  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:55  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:55  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:55  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:55  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:25:55  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:25:55  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:25:55  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:55  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:25:55  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:59  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:59  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:59  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:59  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:59  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:59  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:59  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:59  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:25:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:25:59  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:25:59  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:25:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:25:59  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:25:59  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D7B0040>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CC52D90>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCD1940>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E7F63A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A47CBA100>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:02  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D7B0340>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a68c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:34 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a68c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:34 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a68c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:03  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:06  ->  15 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:06  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:06  ->  15 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:06  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:06  ->  15 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:06  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:09  ->  Getting page: https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:09  ->  Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:09  ->  Getting page: https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:09  ->  Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:09  ->  Getting page: https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:09  ->  Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D307C10>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB641C0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB5CDF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D307CD0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCE1E50>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB5CE50>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:41 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-b0fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:10  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:10  ->  Response: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:41 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-b0fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:10  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:10  ->  Response: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:41 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-b0fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:10  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:10  ->  Response: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:10  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:10  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:10  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:10  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:10  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:10  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:10  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:10  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:10  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:13  ->  17 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:13  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:13  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:13  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:13  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:13  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:13  ->  17 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:13  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:13  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:13  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:13  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:13  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:13  ->  17 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:13  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:13  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:13  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:13  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:13  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:15  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:15  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:15  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:15  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:15  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:15  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:15  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:15  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D70AAF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D307E80>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE01EE0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CC527F0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCE6820>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:16  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D1B7C40>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:47 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c20d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:16  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:16  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:47 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c20d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:16  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:16  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:16  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c20d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:17  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:20  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:20  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:20  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:20  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:20  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:21  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:21  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:21  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:21  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:21  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:21  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:21  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:21  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:21  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:21  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:23  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:23  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:23  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:23  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D64F340>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:23  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:23  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CFD07C0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CF42C40>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:23  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:23  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:23  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:23  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:23  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D4568E0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:23  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:24  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CFC7190>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:55 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c166"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:24  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:24  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:24  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D461D00>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:55 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c166"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:24  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:24  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:24  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:24  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:26:55 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c166"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:24  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:24  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:24  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:24  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:24  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:24  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:24  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:24  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:24  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:28  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:28  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:28  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:28  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:28  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:28  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:28  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:28  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:28  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:28  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:28  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:28  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:30  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D0B8280>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D0B8220>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D0B0850>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CF42A60>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D451100>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:31  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D436490>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c12d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c12d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c12d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:32  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:32  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:32  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:32  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:32  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:32  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:32  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:32  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:32  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:32  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:32  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:32  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:32  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:32  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:35  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:35  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:35  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:35  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:35  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:35  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:35  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:35  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:35  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D451760>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3AEAF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CC86430>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3AEA60>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3B45B0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:38  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D22B0A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:38  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6b20"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6b20"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6b20"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:39  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:40  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:40  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:40  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:40  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:40  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:40  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:40  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:40  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:40  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:40  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:40  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:40  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c2c8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c2c8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c2c8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:43  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:47  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:47  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:47  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:48  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:48  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:48  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:48  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:48  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:48  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:48  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:48  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:48  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:26:48  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3666A0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E8EEFA0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D366FD0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:50  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:51  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E99B790>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:51  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E9A2430>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:26:51  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D4EBFA0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:22 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-83fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:51  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:51  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:22 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-83fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:51  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:51  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:22 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-83fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:51  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:51  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:51  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:51  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:51  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:51  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:51  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:51  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:51  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:51  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:51  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:53  ->  9 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:53  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:53  ->  9 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:53  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:53  ->  9 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:53  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:26:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:26:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78d5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78d5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78d5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:26:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:26:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:26:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:26:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:26:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:26:56  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:58  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:58  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:58  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:58  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:26:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:26:58  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:26:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:26:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:26:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:26:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:26:58  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:00  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:00  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:00  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:00  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:00  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:00  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:00  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:32 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cdf5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:01  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:01  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:32 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cdf5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:01  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:01  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:32 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cdf5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:01  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:01  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:01  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:01  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:01  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:01  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:01  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:01  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:01  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:01  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:01  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:01  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:01  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:01  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:01  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:01  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:02  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:02  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:06  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:06  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:06  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:06  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:06  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:06  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:06  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:06  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:06  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D345E20>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CEF4E50>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB36DC0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D348850>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D37FD60>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D348CA0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccf0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccf0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccf0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:10  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:10  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:10  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:10  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:10  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:10  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:10  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:10  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:10  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:10  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:10  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:10  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:14  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:14  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:14  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:14  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:14  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:14  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:14  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:14  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:14  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:14  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:14  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:14  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:16  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE2E4F0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CEF43D0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CF34520>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CF34370>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D0AE940>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:17  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D345DC0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccb1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccb1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccb1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:18  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:18  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:18  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:18  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:18  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:18  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:18  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:18  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:18  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:18  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:18  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:21  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:21  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:21  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:21  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:21  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:22  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:22  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:22  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:22  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:22  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:22  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:22  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:22  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D426D60>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D411190>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D781550>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:24  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:24  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:24  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:24  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:24  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D4946A0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:24  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:25  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D4118E0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cd3a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:25  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE03CA0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cd3a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:25  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:25  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:27:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cd3a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:25  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:25  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:25  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:25  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:25  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:25  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:25  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:29  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:29  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:29  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:29  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:29  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:29  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:29  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:29  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:29  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:29  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:29  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:29  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:29  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:29  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:29  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:31  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D780C10>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D49BB20>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D497430>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5ADCA0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D426970>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:32  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CDE01C0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:03 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cb93"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:32  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:32  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:03 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cb93"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:32  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:32  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:03 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cb93"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:32  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:32  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:32  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:32  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:32  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:32  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:32  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:32  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:32  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:32  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:32  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:32  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:32  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:32  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:32  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:32  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:33  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:33  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:33  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:33  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:33  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:33  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:33  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:33  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:33  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:36  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:36  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:36  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:36  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:36  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:36  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:36  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:36  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:36  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:36  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:36  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:36  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:36  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:36  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:36  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:38  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:38  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:39  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:39  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5ADFA0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D6FE670>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5ADDF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D6FEB20>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D6FE520>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8a25"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:39  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CF380A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8a25"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:39  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:40  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8a25"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:40  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:40  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:40  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:40  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:40  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:40  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:40  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:40  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:42  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:42  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:42  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:42  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:27:42  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:27:42  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:42  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:42  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:42  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:42  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:27:42  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:27:42  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:42  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:42  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:42  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:42  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:27:42  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:27:42  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:44  ->  Getting page: https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:44  ->  Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:44  ->  Getting page: https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:44  ->  Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:44  ->  Getting page: https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:44  ->  Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:44  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9d01"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:45  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:45  ->  Response: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9d01"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:45  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:45  ->  Response: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9d01"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:45  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:45  ->  Response: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:45  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:45  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:45  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:45  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:45  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:45  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:45  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:45  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:45  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:47  ->  13 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:27:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:27:47  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:47  ->  13 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:27:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:27:47  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:47  ->  13 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:27:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:27:47  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c912"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c912"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c912"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:50  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:51  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:51  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:51  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:51  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:51  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:51  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:51  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:51  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:54  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:54  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:54  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:54  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:54  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:54  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:27:54  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:27:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:27:54  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:27:54  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:57  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:57  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:57  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:27:57  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:27:57  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D0AE430>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D68C850>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EA18F10>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D4E11C0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D68C340>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:27:57  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D4F4BE0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6aa"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6aa"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6aa"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:27:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:27:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:27:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:27:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:27:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:27:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:27:58  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:27:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:02  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:02  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:02  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:02  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:02  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:02  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:02  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:02  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:02  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:02  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:02  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:02  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:04  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:04  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:04  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:04  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:04  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:04  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EA186A0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EA10A00>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:04  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:05  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EB18AF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:05  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:05  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E9DC340>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:05  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EA184F0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:05  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EB18E20>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c688"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c688"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c688"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:05  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:06  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:09  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:09  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:09  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:09  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:09  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:09  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:09  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:09  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:09  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:09  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:09  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:09  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E9DF670>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EBC7C10>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E9DF700>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D070250>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EB24280>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:12  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EC7B730>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:44 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c816"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:44 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c816"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:44 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c816"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:13  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:17  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:17  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:17  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:17  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:17  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:17  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:17  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:17  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:17  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:17  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:17  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:17  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:19  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:19  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:19  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:19  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:19  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:19  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:19  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EF40F70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4ED2A970>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EE90520>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EF409A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EDE6910>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:20  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EC7B3D0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:52 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c753"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:21  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:21  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:52 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c753"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:21  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:21  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:52 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c753"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:21  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:21  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:21  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:21  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:21  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:21  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:21  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:21  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:21  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:21  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:21  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:24  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:24  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:24  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:24  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:24  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:25  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:25  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:25  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:25  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:25  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:25  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:25  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:25  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:25  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:25  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F0A4F10>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F0A4E20>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:27  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:28  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EDDBAF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:28  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:28  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EF4C250>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:28  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F0A4FA0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:28  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F0B0070>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c498"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c498"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:28:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c498"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:28  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:29  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:29  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:29  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:29  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:29  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:29  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:29  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:29  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:29  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:29  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:29  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:29  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:32  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:32  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:32  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:32  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:32  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:32  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:32  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:32  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:32  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:32  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:32  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:32  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:32  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:32  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:33  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:35  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:35  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:35  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:35  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:35  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:35  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F208B20>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F20F130>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EDDBF70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:35  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:36  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D68C250>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:36  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F0A4A60>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:36  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D4E1070>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:07 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c5ec"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:36  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:36  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:07 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c5ec"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:36  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:36  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:07 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c5ec"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:36  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:36  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:36  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:40  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:40  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:40  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:40  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:40  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:40  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:40  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:40  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:28:41  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:43  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:43  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:43  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:43  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:43  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:43  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E8A7B20>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F20F460>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D0AE520>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CDE0FD0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:43  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CDE0E50>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:43  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:44  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EFF9280>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:15 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9417"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:44  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:44  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:15 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9417"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:44  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:44  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:15 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9417"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:44  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:44  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:44  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:44  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:44  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:44  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:44  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:44  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:44  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:44  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:46  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:46  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:46  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:46  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:46  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:46  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:46  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:46  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:46  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:46  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:46  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:46  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:47  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:47  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:49  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:49  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:49  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:20 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ad9f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:49  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:49  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:20 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ad9f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:49  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:49  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:49  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:49  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D6C2A30>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:49  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:50  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D68C070>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ad9f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:50  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:53  ->  16 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:53  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:53  ->  16 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:53  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:54  ->  16 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:54  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:54  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:54  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CDF7C10>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D441310>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:28:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:28:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CDF76A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CDE0220>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D441430>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:56  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6ba6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6ba6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:57  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:57  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:28:57  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D79AA60>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:57  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:57  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6ba6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:28:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:28:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:28:57  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:28:57  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:28:57  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:57  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:28:57  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:58  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:58  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:28:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:58  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:58  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:28:58  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:28:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:28:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:28:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:28:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:28:58  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:00  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:00  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:00  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:00  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:31 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6f3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:00  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:00  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:00  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:01  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:01  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:32 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6f3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:01  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:01  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:01  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:01  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:32 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6f3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:01  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:01  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:01  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:01  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:01  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:01  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:01  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:01  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:01  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:06  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:06  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:06  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:06  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:06  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:06  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:06  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:06  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:06  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D2D2400>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D2D29D0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:08  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:09  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D495370>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:09  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D592370>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5921F0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5E98B0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3c7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3c7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3c7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:09  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:10  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:10  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:10  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:10  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:10  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:10  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:14  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:14  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:14  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:14  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:14  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:14  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:14  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:14  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:14  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:14  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:14  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:14  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:16  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:16  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:16  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:16  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:16  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:16  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:16  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:17  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:17  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:17  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:17  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3C1D90>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E862FA0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3C1970>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D06F370>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D705AC0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:17  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D06F460>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c730"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c730"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c730"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:18  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:18  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:18  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:18  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:18  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:18  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:18  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:18  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:22  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:22  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:22  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:22  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:22  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:22  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:22  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:22  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:22  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:22  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:22  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:22  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:24  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:24  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:24  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:24  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:24  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:24  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:24  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:24  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:25  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:25  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE82D00>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D495D60>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E808EE0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D31B0A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D676CA0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:25  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F333CD0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7659"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7659"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:29:57 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7659"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:26  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:26  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:26  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:26  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:26  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:26  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:26  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:26  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:26  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:26  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:26  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:27  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:27  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:27  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:27  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:27  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:27  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:28  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:28  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:28  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:28  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:28  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:28  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:28  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:28  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:28  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:28  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c830"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c830"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c830"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:31  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:35  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:35  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:35  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:36  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:36  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:36  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:36  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:36  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:36  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:36  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:36  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:36  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:36  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F45EDF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F44D8B0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:38  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:39  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F33A280>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:39  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:39  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F33A3D0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:39  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4F333AF0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c68f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:39  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D598400>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c68f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:39  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:40  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c68f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:40  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:40  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:40  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:40  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:40  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:40  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:40  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:40  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:43  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:43  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:43  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:43  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:43  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:43  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:43  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:43  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:29:43  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:46  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:46  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:46  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:46  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:46  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:46  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D598190>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCC9A30>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:46  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:47  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCC3D30>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:47  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:47  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CB40310>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:47  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D10B130>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:29:47  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CD58AC0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7dd2"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:47  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:47  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7dd2"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:47  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:47  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:47  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:47  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:47  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:47  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:47  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:47  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7dd2"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:47  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:47  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:47  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:48  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:48  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:48  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:48  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:48  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:48  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:48  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:48  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:48  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:48  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:48  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:48  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:48  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:48  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:48  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:48  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:49  ->  8 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:49  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:49  ->  8 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:49  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:49  ->  8 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:49  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:51  ->  Getting page: https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:51  ->  Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:51  ->  Getting page: https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:51  ->  Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:51  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:52  ->  Getting page: https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:52  ->  Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-703c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-703c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-703c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:52  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:52  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:52  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:52  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:52  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:52  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:52  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:52  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:52  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:52  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:52  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:52  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:52  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:53  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:53  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:53  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:53  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:53  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:53  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:53  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:54  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:54  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:54  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:54  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:54  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:54  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:54  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:54  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:29:54  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:29:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:29:54  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:29:54  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:29:54  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:29:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:29:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3f7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3f7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3f7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:29:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:29:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:57  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:57  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:57  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:57  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:29:57  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:29:57  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:29:57  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:29:57  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:29:57  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:29:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:01  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:01  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:01  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:01  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:30:01  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:01  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:01  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:01  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:01  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:30:01  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:01  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:01  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:01  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:01  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:30:01  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:03  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:03  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:03  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:03  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:03  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:03  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:03  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D67D5B0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE7C0D0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D02C7F0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE569A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE56FA0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:04  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D67D280>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:35 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c45c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:04  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:04  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:35 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c45c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:04  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:04  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:35 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c45c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:04  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:04  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:04  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:05  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:09  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:09  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:09  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:30:09  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:09  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:09  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:09  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:30:09  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:09  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:09  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:09  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:30:09  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:11  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:11  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:11  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:11  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:11  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:11  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:11  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE8BEE0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D105F70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D111400>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D105DC0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE8BFD0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:12  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D105820>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9fbd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9fbd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9fbd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:13  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:16  ->  14 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:16  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:16  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:16  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:16  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:16  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:16  ->  14 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:16  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:16  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:16  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:16  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:16  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:16  ->  14 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:16  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:16  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:16  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:16  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:16  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:18  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:18  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:18  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:18  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:18  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:18  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:18  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3BCF70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D1BEE50>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3BC880>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3B13D0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3BC940>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:19  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CF602E0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:50 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a633"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:19  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:19  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:50 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a633"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:19  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:19  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:50 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a633"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:19  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:19  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:19  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:20  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:23  ->  14 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:23  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:23  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:23  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:23  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:23  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:23  ->  14 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:23  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:23  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:23  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:23  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:23  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:23  ->  14 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:23  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:23  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:23  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:23  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:23  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:26  ->  Getting page: https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:26  ->  Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:26  ->  Getting page: https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:26  ->  Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:26  ->  Getting page: https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:26  ->  Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3BC8B0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D1BE3A0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D22AA30>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D105760>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:26  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE8BFA0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:26  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:27  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D612610>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:58 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bc4f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:27  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:27  ->  Response: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:58 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bc4f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:27  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:27  ->  Response: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:30:58 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bc4f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:27  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:27  ->  Response: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:27  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:27  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:27  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:27  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:27  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:27  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:27  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:28  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:31  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:31  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:31  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:31  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:31  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:31  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:31  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:31  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:31  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:31  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:31  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:31  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:31  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:31  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:31  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:31  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:31  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:31  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:33  ->  Getting page: https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:33  ->  Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:33  ->  Getting page: https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:33  ->  Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:33  ->  Getting page: https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:33  ->  Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:33  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CF5DB80>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D307550>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EC62FA0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EDE6AC0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4C8D3730>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:30:34  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4EC62820>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:34  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5389"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:35  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:35  ->  Response: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:35  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:35  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5389"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:35  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:35  ->  Response: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5389"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:35  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:35  ->  Response: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:35  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:35  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:35  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:35  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:35  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:35  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:35  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:35  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:35  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:35  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:35  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:35  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:35  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:35  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:35  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:35  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:35  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:35  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:38  ->  Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:09 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7c19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:38  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:38  ->  Response: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:38  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:38  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:09 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7c19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:38  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:38  ->  Response: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:09 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7c19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:38  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:38  ->  Response: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:38  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:38  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:38  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:38  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:38  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:38  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:38  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:40  ->  8 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:40  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:40  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:40  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:40  ->  8 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:40  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:40  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:40  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:40  ->  8 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:40  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:40  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:40  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:42  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:43  ->  Getting page: https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:43  ->  Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78f8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78f8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78f8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:43  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:44  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:44  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:44  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:44  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:44  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:44  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:45  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:45  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:45  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:45  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:45  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:45  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:45  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:45  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:45  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:45  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:45  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:45  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:47  ->  Getting page: https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:47  ->  Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:47  ->  Getting page: https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:47  ->  Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:47  ->  Getting page: https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:47  ->  Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-810a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:47  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:47  ->  Response: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-810a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:47  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:47  ->  Response: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:47  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:48  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:48  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:48  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-810a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:48  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:48  ->  Response: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:48  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:48  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:48  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:48  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:48  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:48  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:48  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:48  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:48  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:48  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:48  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:48  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:48  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:49  ->  9 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:49  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:49  ->  9 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:49  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:50  ->  9 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:50  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:50  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:50  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:50  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:52  ->  Getting page: https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:52  ->  Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:52  ->  Getting page: https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:52  ->  Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:52  ->  Getting page: https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:52  ->  Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:52  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:52  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:52  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:52  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:52  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:52  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:52  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:52  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:52  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:52  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:52  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:52  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:52  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:52  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:53  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:53  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:53  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:53  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:53  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:53  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:53  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:53  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:53  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:53  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:53  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5381"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5381"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5381"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:56  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:56  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:56  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:56  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:56  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:30:56  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:56  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:30:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:30:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:30:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:30:56  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:30:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:30:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ad6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ad6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ad6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:30:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:30:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:30:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:30:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:30:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:30:59  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:30:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:30:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:00  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:00  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:00  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:00  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:00  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:00  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:00  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:00  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:00  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:00  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:00  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:00  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:00  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:01  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:01  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:01  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:01  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:01  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:01  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:02  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:02  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:02  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:02  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:02  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:02  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:02  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:02  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:02  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:02  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:04  ->  Getting page: https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:04  ->  Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:04  ->  Getting page: https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:04  ->  Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:04  ->  Getting page: https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:04  ->  Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:35 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-adde"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:04  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:04  ->  Response: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:35 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-adde"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:04  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:04  ->  Response: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:04  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-adde"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:05  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:10  ->  17 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:10  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:10  ->  17 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:10  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:10  ->  17 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:10  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:12  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:12  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:12  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:12  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:12  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:12  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:12  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:13  ->  Getting page: https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:13  ->  Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:13  ->  Getting page: https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:13  ->  Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:13  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:13  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:13  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D7A9FD0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:13  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:13  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5F2AC0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:13  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:13  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CD714F0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:13  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:14  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D61E1F0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:14  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5CC790>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:14  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5F25B0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:45 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c096"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:14  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:14  ->  Response: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:45 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c096"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:14  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:14  ->  Response: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:45 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c096"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:14  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:14  ->  Response: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:14  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:14  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:14  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:14  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:14  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:14  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:14  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:14  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:14  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:15  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:15  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:15  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:15  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:15  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:15  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:15  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:15  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:15  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:15  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:15  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:15  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:18  ->  18 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:18  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:18  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:18  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:18  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:18  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:18  ->  18 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:18  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:18  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:18  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:18  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:18  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:18  ->  18 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:18  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:18  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:18  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:18  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:18  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:21  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:21  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:21  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:21  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:21  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:21  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D040400>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D607F70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D33C700>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D603400>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:21  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D24E640>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:21  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:21  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:22  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D37ED00>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:53 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-d36c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:22  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:22  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:53 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-d36c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:22  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:22  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:31:53 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-d36c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:22  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:22  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:22  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:22  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:22  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:22  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:22  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:22  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:22  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:23  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:23  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:23  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:23  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:23  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:26  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:26  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:26  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:26  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:31:26  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:27  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:27  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:27  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:27  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:31:27  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:27  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:27  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:27  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:27  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:31:27  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D0488B0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:29  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:30  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CC3DA60>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:30  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:30  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CFF0E50>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:30  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:30  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CC3DA90>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:30  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D448730>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:30  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CC3DC10>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8e9e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8e9e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:30  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:30  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:30  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:30  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:30  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:30  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:30  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:30  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:30  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:30  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:30  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:30  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8e9e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:31  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:33  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:33  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:33  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:33  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:33  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:33  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:33  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:33  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:33  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:33  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:33  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:33  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:33  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:33  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:33  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:33  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:33  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:33  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:35  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:35  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:35  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:35  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:35  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:35  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:35  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:36  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:36  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:36  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:07 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-705f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:36  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:36  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:07 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-705f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:36  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:36  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:36  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CCD1A30>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:36  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:36  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:36  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D5F2F10>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:36  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:37  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:08 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-705f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:37  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:37  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:37  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:37  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:37  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:37  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:37  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:37  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:37  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:37  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:37  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:37  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:37  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:37  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:37  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:37  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:37  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:37  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:37  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:37  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:37  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:37  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:37  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:37  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:37  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:37  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:37  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:37  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:37  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:37  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:37  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:38  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:38  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:38  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:38  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:38  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:38  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:40  ->  Getting page: https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:40  ->  Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:40  ->  Getting page: https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:40  ->  Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:11 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-985e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:40  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:40  ->  Response: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:11 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-985e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:40  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:40  ->  Response: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:40  ->  Getting page: https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:40  ->  Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:40  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:40  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:40  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:41  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:12 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-985e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:41  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:41  ->  Response: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:41  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:41  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:41  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:41  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:41  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:41  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:41  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:41  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:41  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:41  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:41  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:41  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:41  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:41  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:41  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:41  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:41  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:43  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:43  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:43  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:43  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:43  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:43  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:43  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:43  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:43  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:43  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:43  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:43  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:45  ->  Getting page: https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:45  ->  Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:45  ->  Getting page: https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:45  ->  Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:45  ->  Getting page: https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:45  ->  Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:45  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d54"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:46  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:46  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d54"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:46  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:46  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:17 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d54"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:46  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:46  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:46  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:46  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:46  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:47  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:47  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:47  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:47  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:47  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:47  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ce3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ce3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:51  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:51  ->  Response: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ce3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:51  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:51  ->  Response: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:51  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:51  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  response_closed.started
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:51  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:51  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:51  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:51  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:51  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:55  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:55  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:55  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:55  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:55  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:55  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:55  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:55  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:55  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:55  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:55  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:55  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:55  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:55  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:55  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:55  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:55  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:55  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:57  ->  Getting page: https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:57  ->  Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:57  ->  Getting page: https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:57  ->  Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:31:57  ->  Getting page: https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:31:57  ->  Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CF35D30>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CEE7C10>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CDF6D00>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:57  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:58  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D7B6B50>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:29 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5f19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:58  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:58  ->  Response: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:58  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E875730>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:31:58  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4CE2AE50>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:29 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5f19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:58  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:58  ->  Response: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:29 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5f19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:31:58  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:31:58  ->  Response: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:31:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:31:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:31:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:31:58  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:58  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:58  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:31:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:59  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:59  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:59  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:59  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:59  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:31:59  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:31:59  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:31:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:31:59  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:31:59  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:31:59  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:01  ->  Getting page: https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:01  ->  Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:32 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-73d1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:01  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:01  ->  Response: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:01  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:01  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:01  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:01  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:01  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:01  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:01  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:01  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:01  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:01  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:01  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:01  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:01  ->  Executing save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:01  ->  Getting page: https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:01  ->  Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:01  ->  Getting page: https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:01  ->  Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:01  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-73d1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-73d1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:02  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:02  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:02  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:02  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:02  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:02  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:02  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:03  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:03  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:03  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:03  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:03  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:03  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:03  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:03  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:03  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:03  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:03  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:03  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:05  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:05  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-536f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:05  ->  Executing save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:05  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:05  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:05  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:06  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:37 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-536f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:06  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:06  ->  Response: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:37 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-536f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:06  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:06  ->  Response: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:06  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:06  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:06  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:06  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:06  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:39 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d5d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:08  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:08  ->  Response: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:08  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:08  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:08  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:08  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:08  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:08  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:08  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:08  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:08  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:08  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:08  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:08  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:08  ->  Executing save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:08  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d5d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d5d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:09  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:09  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:09  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:09  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:09  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:09  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:10  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:10  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:10  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:10  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-598c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:12  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:12  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:12  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:12  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:12  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:12  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:12  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:12  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:12  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:12  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:12  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:12  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:12  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:12  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:12  ->  Executing save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:12  ->  2 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:12  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:12  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:12  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:12  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:12  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-598c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:44 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-598c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:13  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:13  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:13  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:13  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:13  ->  2 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:13  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:13  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:13  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:13  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:13  ->  2 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:13  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:13  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:13  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:13  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:13  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:15  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:15  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:46 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6056"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:15  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:15  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:15  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:15  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:15  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:15  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:15  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:15  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:15  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:15  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:15  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:15  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:15  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:15  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:15  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:15  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:16  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:16  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:16  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:16  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:16  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:16  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:47 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6056"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:16  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:16  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:16  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:47 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6056"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:17  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:17  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:17  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:17  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:17  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:17  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:17  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:17  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:17  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:17  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:18  ->  Getting page: https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:18  ->  Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:18  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:18  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:18  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:18  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:18  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:19  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:49 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5372"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:19  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:19  ->  Response: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:19  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:19  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:19  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:19  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:19  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:19  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:19  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:19  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:19  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:19  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:19  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:19  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:19  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:19  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:19  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:19  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:19  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:19  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:19  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:19  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:19  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:19  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:19  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:20  ->  Getting page: https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:20  ->  Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:20  ->  Getting page: https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:20  ->  Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:51 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5372"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:20  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:20  ->  Response: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:51 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5372"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:20  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:20  ->  Response: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:20  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:20  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:20  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:20  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:20  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:20  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:20  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:20  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:20  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:20  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:20  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:20  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:21  ->  Getting page: https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:21  ->  Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:21  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:21  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:21  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:21  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:21  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:22  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:53 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5310"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:22  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:22  ->  Response: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:22  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:22  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:22  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:22  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:22  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:22  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:22  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:22  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:22  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:22  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:22  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:22  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:22  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:22  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:22  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:22  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:22  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:23  ->  Getting page: https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:23  ->  Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:23  ->  Getting page: https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:23  ->  Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:54 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5310"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:23  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:23  ->  Response: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:54 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5310"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:23  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:23  ->  Response: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:23  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:23  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:23  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:23  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:23  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:23  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:23  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:24  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:24  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:24  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:24  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:24  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:24  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:24  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:24  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:24  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:24  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:24  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:24  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:24  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:24  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:24  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:24  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:24  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53d0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:25  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:25  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:25  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:25  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:25  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:25  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:25  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:25  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:25  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:25  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:25  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:25  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:25  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:25  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:25  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:25  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:25  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:25  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:25  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:25  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:25  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:25  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:26  ->  Getting page: https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:26  ->  Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:26  ->  Getting page: https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:26  ->  Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:26  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:57 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53d0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:27  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:27  ->  Response: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:58 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53d0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:27  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:27  ->  Response: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:27  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:27  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:27  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:27  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:27  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:27  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:27  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:27  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:27  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:27  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:27  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:27  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:27  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:27  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:27  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:27  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:27  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:32:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6680"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:28  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:28  ->  4 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:28  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:28  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:28  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:28  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:29  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:29  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:29  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:29  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:29  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:29  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:29  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:29  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6680"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6680"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:30  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:30  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:30  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:30  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:31  ->  4 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:31  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:31  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:31  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:31  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:31  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:31  ->  4 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:31  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:31  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:31  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:31  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:31  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-608d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:31  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:32  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:32  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:32  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:32  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:32  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:32  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:33  ->  Getting page: https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:33  ->  Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:33  ->  Getting page: https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:33  ->  Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:33  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:05 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-608d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:34  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:34  ->  Response: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:34  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:34  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:05 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-608d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:34  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:34  ->  Response: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:34  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:34  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:34  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:34  ->  Getting page: https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:34  ->  Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:34  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:34  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:34  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:34  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:34  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:34  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:34  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:05 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5315"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:35  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:35  ->  Response: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:35  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:35  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:35  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:35  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:35  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:35  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:35  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:35  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:35  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:35  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:35  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:35  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:35  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:35  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:35  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:35  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:35  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:35  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:35  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:35  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:35  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:35  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:35  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:35  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:35  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:37  ->  Getting page: https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:37  ->  Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:37  ->  Getting page: https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:37  ->  Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:37  ->  Getting page: https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:37  ->  Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:37  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:08 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5315"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:37  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:37  ->  Response: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:37  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:37  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:08 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5315"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:37  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:37  ->  Response: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:37  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:37  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:37  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:37  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:37  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:38  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:38  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:38  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:38  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:38  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:38  ->  Moving on!
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:38  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4E875BE0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:38  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x0000013A4C82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:38  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:38  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:38  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:38  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:38  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:38  ->  Moving on!
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:38  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x0000013A4D3CDF40>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:09 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5300"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:38  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:38  ->  Response: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:38  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:38  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:38  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:38  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:38  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:38  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:38  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:38  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:38  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:38  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:38  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:38  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:38  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:38  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:38  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:38  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:38  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:38  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:38  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:38  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:40  ->  Getting page: https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:40  ->  Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:40  ->  Getting page: https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:40  ->  Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:11 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5300"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:40  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:40  ->  Response: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:11 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5300"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:40  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:40  ->  Response: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:40  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:41  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:41  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:41  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:41  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:41  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:41  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:41  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:41  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:41  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:41  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:41  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:41  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:41  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:41  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:41  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:41  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:41  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:41  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:41  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:41  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:41  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:41  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:41  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:41  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:41  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:41  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:41  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:41  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:41  ->  Getting page: https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:41  ->  Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:12 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:41  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:41  ->  Response: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:41  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:42  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:42  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:42  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:42  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:42  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:42  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:42  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:42  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:42  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:42  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:42  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:42  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:42  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:42  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:42  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:42  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:42  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:42  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:42  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:42  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:42  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:42  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:32:42  ->  Worker-2 completed task in time:  489.98s
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:43  ->  Getting page: https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:43  ->  Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:43  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:43  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:32:43  ->  Getting page: https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:32:43  ->  Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:43  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:15 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:44  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:44  ->  Response: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:33:15 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:32:44  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:32:44  ->  Response: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:44  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:44  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:32:44  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:32:44  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:32:44  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:32:44  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:32:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:44  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:44  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:44  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:44  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:44  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:32:44  ->  Worker-1 completed task in time:  492.25s
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:32:44  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:32:44  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:32:44  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:32:44  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:32:44  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:32:44  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:32:44  ->  Worker-3 completed task in time:  492.31s
[INFO|beautifulcrawler|WEB_SCRAPER|L433] - 2024-06-06 23:32:44  ->  Worker(s) finished all work in time: 492.32s
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:44  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:44  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:44  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:32:44  ->  close.complete
[INFO|utils|numexpr.utils|L160] - 2024-06-06 23:36:50  ->  NumExpr defaulting to 4 threads.
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:36:51  ->  Using proactor: IocpProactor
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:36:51  ->  Using proactor: IocpProactor
[DEBUG|_config|httpx|L80] - 2024-06-06 23:36:51  ->  load_ssl_context verify=True cert=None trust_env=True http2=False
[DEBUG|_config|httpx|L146] - 2024-06-06 23:36:51  ->  load_verify_locations cafile='C:\\Users\\rm\\anaconda3\\envs\\DataEngineeringEnv\\Library\\ssl\\cacert.pem'
[ERROR|storage|WEB_SCRAPER|L26] - 2024-06-06 23:36:52  ->  Invalid repository type selected
[INFO|beautifulcrawler|WEB_SCRAPER|L424] - 2024-06-06 23:36:52  ->  Worker(s) starting work...
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:36:52  ->  Worker-1 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:36:52  ->  Worker-2 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:36:52  ->  Worker-3 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:36:55  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:36:55  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:36:55  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:36:55  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:36:55  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:36:55  ->  Request: GET https://books.toscrape.com
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001844D9CB520>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001844D9DC0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001844DA1E0A0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001844D9DC0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001844DA704C0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:55  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001844D9DC0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:56  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001844DA1E220>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:56  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001844DA706A0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:36:56  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001844DA87A00>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:36:56  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:36:56  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:36:56  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:36:56  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:36:56  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:36:56  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:36:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:36:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:36:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:36:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:36:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:36:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:36:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:36:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:36:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:36:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3bd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:36:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:36:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:36:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:36:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:36:59  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:36:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:36:59  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:36:59  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:36:59  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:36:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:36:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:36:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:36:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:36:59  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:36:59  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:00  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:00  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:00  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:00  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:00  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:00  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:00  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:00  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:00  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:00  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:00  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:00  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:00  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:00  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:00  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:00  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:00  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:00  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:00  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:00  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:00  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:00  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:00  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8ff"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:02  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7157"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:03  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:03  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:03  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:03  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:03  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:03  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:03  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:03  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:03  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:03  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:03  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:03  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:03  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:03  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:03  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:03  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:03  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:03  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:37 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c7dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:37 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bb02"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:06  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:06  ->  Response: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:06  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:06  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:06  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:37 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-90a8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:06  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:06  ->  Response: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:06  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:06  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:06  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:06  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:06  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:06  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:08  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8c0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:09  ->  Getting page: https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:09  ->  Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:09  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:09  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:09  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-b0fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:09  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:09  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:09  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:09  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:09  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:09  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:09  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:09  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:09  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ab40"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:12  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:12  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a68c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:12  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:12  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:12  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:12  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:12  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:12  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:12  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:12  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:12  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c20d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:12  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:12  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:12  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:12  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:12  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:12  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:12  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:12  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:12  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:12  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:12  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:12  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:13  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:13  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:13  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:13  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:13  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:13  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:13  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:13  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:13  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:13  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:13  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:13  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:13  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:13  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:13  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:13  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:13  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:13  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:15  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:15  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:15  ->  Getting page: https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:15  ->  Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:46 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c2c8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:15  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:15  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:15  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:15  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:46 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78d5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:15  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:15  ->  Response: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:15  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:15  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:15  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:15  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:15  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:15  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:15  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:15  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:15  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:15  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:15  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:15  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:15  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:15  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:15  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:15  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:15  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:15  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:15  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:15  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:15  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:15  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:15  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:16  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:16  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:16  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:16  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:16  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:46 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c166"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:16  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:16  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:16  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:16  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:16  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:16  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:16  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:16  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:16  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:16  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:16  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:16  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:16  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:16  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:18  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:18  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:18  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:18  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:49 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cdf5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:18  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:18  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:49 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-83fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:18  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:18  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:18  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:18  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:18  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:18  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:18  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:18  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:18  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:18  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:19  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:19  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:19  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:19  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:19  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:19  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:19  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:19  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:19  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:19  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:19  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:19  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:19  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:19  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:19  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:50 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c12d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:19  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:19  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:19  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:19  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:19  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:19  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:19  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:19  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:19  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:19  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:19  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:19  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:19  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:19  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:21  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:21  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:21  ->  Getting page: https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:21  ->  Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:52 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccf0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:21  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:21  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:52 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9d01"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:21  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:21  ->  Response: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:21  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:22  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:22  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:22  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:22  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:22  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:22  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:22  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:22  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:22  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:22  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:22  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:22  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:22  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:53 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6b20"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:22  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:22  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:22  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:22  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:22  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:22  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:22  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:22  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:22  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:22  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:22  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:22  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:24  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccb1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c912"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ad9f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:25  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:25  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:25  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:25  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:25  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:25  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:25  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:25  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:25  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:25  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:25  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:25  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:25  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:25  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:25  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:25  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:25  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:25  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:25  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:25  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:25  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:25  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:27  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:27  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:27  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:27  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:27  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:28  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:28  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:28  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:28  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cd3a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6aa"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:28  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:28  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:28  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:37:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6ba6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:28  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:28  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:28  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:28  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:28  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:28  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:28  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:28  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:28  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cb93"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6f3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:31  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:31  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:31  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:31  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c688"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:31  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:31  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:31  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:31  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:31  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:32  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:32  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:32  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:32  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:32  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:32  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:32  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:32  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:32  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:32  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:32  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:32  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:32  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:32  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:32  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:32  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:32  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:32  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:32  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:32  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:34  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:34  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:34  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:34  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:05 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8a25"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:34  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:34  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:34  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:34  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:34  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:34  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:34  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:34  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:34  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:34  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:34  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:34  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:34  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:34  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:34  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:34  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:34  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:34  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:34  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:34  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:34  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:05 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3c7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:34  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:34  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:34  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:34  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:34  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:35  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:35  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:35  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:35  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:35  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:35  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:06 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c816"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:35  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:35  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:35  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:35  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:35  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:35  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:35  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:35  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:35  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:35  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:35  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:35  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:35  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:35  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:37  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:37  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:37:37  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:37:37  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:08 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c830"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:37  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:37  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:37  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:37  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:37  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:37  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:37  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:37  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:37  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:37  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:37  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:37  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:37  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:37  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:37  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:37  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:37  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:37  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:37  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:37  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:37  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:37  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:37  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:37  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:08 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c730"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:37  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:37  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:37  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:38  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:09 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c753"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:38  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:38  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:38  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:38  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:38  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:38  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:38  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:38  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:38  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:38  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:38  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:38  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:38  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:38  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:38  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:38  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:38  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:38  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:38  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:38  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:38  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:38  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:38  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:40  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:40  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:11 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c68f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:40  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:40  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:40  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:40  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:40  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:41  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:41  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:41  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:41  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:41  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:41  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:12 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7659"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:41  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:41  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:41  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:41  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:41  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:41  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:41  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:41  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:41  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:12 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c498"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:41  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:41  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:41  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:41  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:41  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:41  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:41  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:41  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:41  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:41  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:41  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:43  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:43  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:43  ->  Getting page: https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:43  ->  Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7dd2"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:44  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:44  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:44  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:44  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:44  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:15 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-703c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:44  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:44  ->  Response: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:44  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:44  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:44  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:44  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:44  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:44  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:44  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:44  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:44  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:15 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c5ec"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:44  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:44  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:44  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:44  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:44  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:44  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:44  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:44  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:44  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:44  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:44  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:46  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:46  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:46  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:46  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:17 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3f7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:17 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a633"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:47  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:47  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:47  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:47  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:47  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  response_closed.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:47  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:47  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:47  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:47  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:47  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:47  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:47  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:47  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9417"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:47  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:47  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:47  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:48  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:48  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:48  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:48  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:48  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:48  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:48  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:48  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:48  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:48  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:48  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:48  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:48  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:48  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:48  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:48  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:48  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:48  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:48  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:49  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c45c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bc4f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:50  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:50  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:37:50  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:50  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:50  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:50  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:50  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5389"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:50  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:50  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:50  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:50  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:50  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:50  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:52  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:52  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:52  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:52  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:52  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:52  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:52  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:53  ->  Getting page: https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:53  ->  Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:24 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9fbd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:53  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:53  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:24 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7c19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:53  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:53  ->  Response: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:53  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:53  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:53  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:53  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  response_closed.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:53  ->  Getting page: https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:53  ->  Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:53  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:53  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:53  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:53  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:53  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:53  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:24 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78f8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:53  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:53  ->  Response: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:53  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:53  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:53  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:53  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:53  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:54  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:54  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:54  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:54  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:54  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-810a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:56  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:56  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:56  ->  Getting page: https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:56  ->  Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:56  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:56  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5381"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:56  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:56  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:56  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:56  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ad6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:59  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:59  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:59  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:59  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-adde"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:37:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:37:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:37:59  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:37:59  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:37:59  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:37:59  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:37:59  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:37:59  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c096"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:37:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:37:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:37:59  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:37:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:37:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:37:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:00  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:00  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:00  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:00  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:00  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:00  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:00  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:00  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:00  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:00  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:00  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:00  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:00  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:00  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:00  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:00  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:01  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:01  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:01  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:01  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:01  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:01  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:01  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-d36c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:02  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:02  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:38:02  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:33 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-705f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:02  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:02  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:02  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:02  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:02  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:02  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:02  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:02  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:02  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:02  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:02  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:03  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:34 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-985e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:03  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:03  ->  Response: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:03  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:03  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:03  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:03  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:03  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:03  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:03  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:03  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8e9e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:36 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d54"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:05  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:05  ->  Response: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:05  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:05  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:05  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:05  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:05  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:05  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:05  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:05  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:05  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:05  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:05  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:05  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:05  ->  Getting page: https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:05  ->  Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:05  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:06  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:37 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ce3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:06  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:06  ->  Response: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:06  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:06  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:06  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:06  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:06  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:06  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:06  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:06  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:06  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:06  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:06  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:06  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:06  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:06  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:06  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:06  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:06  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:06  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:06  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:06  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:06  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:06  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:39 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5f19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:08  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:08  ->  Response: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:08  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:08  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:08  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:08  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:08  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:08  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:08  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:39 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-73d1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:08  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:08  ->  Response: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:08  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:08  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:08  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:08  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:08  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:08  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:08  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:08  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:08  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:08  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:09  ->  Getting page: https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:09  ->  Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-536f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:09  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:09  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:09  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:09  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:09  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:09  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:09  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:11  ->  Getting page: https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:11  ->  Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:11  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:11  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:42 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-598c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:11  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:11  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:42 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d5d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:11  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:11  ->  Response: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:11  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:11  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:11  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:11  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:11  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:11  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:11  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:11  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:11  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:11  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:11  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:11  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:11  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:11  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:11  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:11  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:11  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:12  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:12  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:43 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6056"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:12  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:12  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:12  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:12  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:12  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:12  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:12  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:12  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:12  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:12  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:12  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:12  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:12  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:12  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:12  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:12  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:12  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:12  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:12  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:12  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:12  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:12  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:14  ->  Getting page: https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:14  ->  Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:14  ->  Getting page: https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:14  ->  Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:45 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5372"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:14  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:14  ->  Response: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:14  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:14  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:14  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:14  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:14  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:14  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:45 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5310"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:14  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:14  ->  Response: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:14  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:14  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:14  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:14  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:14  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:14  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:14  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:14  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:14  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:15  ->  Getting page: https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:15  ->  Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:46 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53d0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:15  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:15  ->  Response: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:15  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:15  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:15  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:15  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:15  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:15  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:15  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:15  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:15  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:15  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:15  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:15  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:15  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:15  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:15  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:15  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:15  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:15  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:15  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:15  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:17  ->  Getting page: https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:17  ->  Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:17  ->  Getting page: https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:17  ->  Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6680"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:17  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:17  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:17  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:17  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-608d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:17  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:17  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:17  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:17  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:17  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:17  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:17  ->  Getting page: https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:17  ->  Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:18  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:49 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5315"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:18  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:18  ->  Response: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:18  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:18  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:18  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:18  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:18  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:18  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:18  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:18  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:18  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:18  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:18  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:18  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:18  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:18  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:18  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:18  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:38:18  ->  Worker-2 completed task in time:  85.45s
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:20  ->  Getting page: https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:20  ->  Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:38:20  ->  Getting page: https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:38:20  ->  Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:51 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5300"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:20  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:20  ->  Response: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:20  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:20  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:20  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:20  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:20  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:38:20  ->  Worker-1 completed task in time:  87.58s
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:38:51 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:38:20  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:38:20  ->  Response: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:38:20  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:38:20  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:38:20  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:38:20  ->  Executing save_to_csv on dataset
[INFO|pipeline_funcs|WEB_SCRAPER|L157] - 2024-06-06 23:38:20  ->  Output/BookScrape.csv saved Successfully!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:38:20  ->  Finished execution of save_to_csv on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:38:20  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:38:20  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:38:20  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:38:20  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:38:20  ->  Worker-3 completed task in time:  87.89s
[INFO|beautifulcrawler|WEB_SCRAPER|L433] - 2024-06-06 23:38:20  ->  Worker(s) finished all work in time: 87.90s
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:38:20  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:38:20  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:38:20  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:38:20  ->  close.complete
[INFO|utils|numexpr.utils|L160] - 2024-06-06 23:39:20  ->  NumExpr defaulting to 4 threads.
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:39:22  ->  Using proactor: IocpProactor
[DEBUG|proactor_events|asyncio|L624] - 2024-06-06 23:39:22  ->  Using proactor: IocpProactor
[DEBUG|_config|httpx|L80] - 2024-06-06 23:39:22  ->  load_ssl_context verify=True cert=None trust_env=True http2=False
[DEBUG|_config|httpx|L146] - 2024-06-06 23:39:22  ->  load_verify_locations cafile='C:\\Users\\rm\\anaconda3\\envs\\DataEngineeringEnv\\Library\\ssl\\cacert.pem'
[INFO|storage|WEB_SCRAPER|L81] - 2024-06-06 23:39:23  ->  Connecting to database mysql
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:39:23  ->  caching sha2: succeeded by fast path.
[INFO|storage|WEB_SCRAPER|L95] - 2024-06-06 23:39:23  ->  Successfully Connected to database!
[INFO|beautifulcrawler|WEB_SCRAPER|L424] - 2024-06-06 23:39:23  ->  Worker(s) starting work...
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:39:23  ->  Worker-1 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:39:23  ->  Worker-2 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L462] - 2024-06-06 23:39:23  ->  Worker-3 crawling BookScrape
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:26  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:26  ->  Request: GET https://books.toscrape.com
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:26  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:26  ->  Request: GET https://books.toscrape.com
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:26  ->  Getting page: https://books.toscrape.com
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:26  ->  Request: GET https://books.toscrape.com
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197E9014F0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197E8E63D0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197E8AB550>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:26  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:27  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197E8AB3D0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:27  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197E8E1670>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:27  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197E8E66A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:39:58 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:27  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:27  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:39:58 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:27  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:27  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:39:58 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c85e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:27  ->  HTTP Request: GET https://books.toscrape.com "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:27  ->  Response: GET https://books.toscrape.com - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  response_closed.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:27  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:30  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:30  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9091"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/travel_2/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4d4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3bd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:30  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:30  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:30  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:30  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:30  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:39:30  ->  caching sha2: succeeded by fast path.
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|connection|aiomysql|L951] - 2024-06-06 23:39:30  ->  caching sha2: succeeded by fast path.
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:33  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:33  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:33  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:33  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:39:33  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:39:33  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:34  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:34  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:34  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:34  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:39:34  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:34  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:34  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:34  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:34  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:39:34  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:35  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:35  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:35  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:35  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:35  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:35  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:35  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:07 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8ff"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:36  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:36  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  response_closed.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:36  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:36  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:36  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:36  ->  close.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:36  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:36  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:36  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:36  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:36  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:36  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:36  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:36  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:36  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:36  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:36  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:36  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:36  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:36  ->  Getting page: https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:36  ->  Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:36  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:36  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:36  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:36  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:08 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9685"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:37  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:37  ->  Response: GET https://books.toscrape.com/catalogue/category/books/mystery_3/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:37  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EC970A0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:37  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:37  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:37  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:37  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EDF9AC0>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:08 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7157"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:37  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:37  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical-fiction_4/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:37  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:37  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:37  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:37  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:37  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:39  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:39  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:39  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:39  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:39:39  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:39:39  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:40  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:40  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:39:40  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:40  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:40  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:39:40  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:39:40  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:13 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bb02"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:42  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:42  ->  Response: GET https://books.toscrape.com/catalogue/category/books/classics_6/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:13 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c7dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:42  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:42  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:42  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:42  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:42  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:42  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:42  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:42  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:42  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:42  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:42  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:42  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:42  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:42  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:42  ->  Executing save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-90a8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/philosophy_7/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:43  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:45  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:45  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:45  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:45  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:39:45  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:39:45  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:45  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:45  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:45  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:45  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:39:45  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:39:45  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:45  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:45  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:45  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:45  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:39:45  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:47  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:47  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:47  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:47  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:47  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:47  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:47  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:47  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c4dd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:47  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:47  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:47  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  response_closed.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:48  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:48  ->  close.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:48  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:48  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:48  ->  Getting page: https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:48  ->  Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:48  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:48  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:48  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:19 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-b0fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:48  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:48  ->  Response: GET https://books.toscrape.com/catalogue/category/books/womens-fiction_9/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:48  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:48  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:48  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:48  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:48  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EC83F40>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:48  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:48  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EE76460>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:48  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:49  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:20 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c8c0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:49  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:49  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:49  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:49  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:49  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:49  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:49  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:49  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:49  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:49  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:49  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:49  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:49  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:49  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:49  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:49  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:49  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:49  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:49  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:52  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:52  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:39:52  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:52  ->  17 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:52  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:39:52  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:39:52  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:39:53  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:53  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:39:53  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:39:53  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:39:53  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EC88F70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EFD3460>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:39:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:39:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:55  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:56  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EDFF910>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:56  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EF56C10>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:56  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197ECE7F40>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:56  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a68c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/romance_8/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:39:56  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EE7D2E0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c20d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:27 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ab40"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:39:56  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:39:56  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sequential-art_5/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:39:56  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:39:56  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:39:56  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:39:56  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:39:56  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:39:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:00  ->  15 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:00  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:00  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:00  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:00  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:00  ->  Moving on!
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:00  ->  15 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:00  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:00  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:00  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:00  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:00  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:00  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:00  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:00  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:00  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:00  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:02  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:02  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:02  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:02  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:02  ->  close.started
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:02  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:02  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:02  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F1BAA00>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EE76670>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:03  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:03  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F125D30>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EC83DF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F125D60>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:34 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c2c8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:03  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:03  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:34 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78d5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:03  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:03  ->  Response: GET https://books.toscrape.com/catalogue/category/books/religion_12/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:03  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F133910>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:03  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:04  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:04  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:04  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:35 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c166"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:04  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:04  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:04  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:04  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:04  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:04  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:04  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:04  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:04  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:04  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:05  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:05  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:05  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:05  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:05  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:07  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:07  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:07  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:07  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:07  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:07  ->  going to next page
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:08  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:08  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:08  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:08  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:08  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:39 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cdf5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:08  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:08  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:08  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:08  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:08  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:08  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:08  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:08  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:08  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:08  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:08  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:08  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:08  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:08  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:08  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:08  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:10  ->  Getting page: https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:10  ->  Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:10  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:10  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:10  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:10  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:10  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:10  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:10  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:41 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-83fc"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:10  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:10  ->  Response: GET https://books.toscrape.com/catalogue/category/books/childrens_11/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:10  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F1C8730>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:10  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:10  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:10  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:10  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:10  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:10  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:10  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:10  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:10  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:10  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:10  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:10  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:10  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:10  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:10  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:11  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F079A00>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:42 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c12d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:11  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:11  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:11  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:11  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:11  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:11  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:11  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:11  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:11  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:11  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:11  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:11  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:11  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:11  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:11  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:11  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:11  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:11  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:11  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:11  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:11  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:12  ->  9 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:12  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:12  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:12  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:12  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:12  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:14  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:14  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:45 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccf0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:14  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:14  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:14  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:14  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:14  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:14  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:14  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:14  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:14  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:14  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:14  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:14  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:14  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:14  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:14  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:14  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:15  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:15  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:15  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:15  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:15  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:15  ->  Getting page: https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:15  ->  Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:46 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9d01"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:15  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:15  ->  Response: GET https://books.toscrape.com/catalogue/category/books/music_14/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:15  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:15  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:15  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:15  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:15  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:15  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:15  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:15  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:15  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:15  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:15  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:15  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:15  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:15  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:17  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:17  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:17  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:17  ->  close.complete
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6b20"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fiction_10/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:17  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:17  ->  13 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:17  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:17  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:17  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:17  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:17  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:18  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:18  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:18  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:18  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:18  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:18  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:18  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:18  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:18  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:18  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:18  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:20  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:20  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:20  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:20  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:20  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:20  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:20  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:20  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:51 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c912"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:20  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:20  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:20  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:20  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:20  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:20  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:21  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:21  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:21  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:21  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:21  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:21  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:21  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:21  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:21  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:21  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:21  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:21  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:21  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:21  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F431F70>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:21  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:21  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:21  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:21  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F2742E0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:52 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ad9f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:21  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:21  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science-fiction_16/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:40:52 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-ccb1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:21  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:21  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:21  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:21  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:21  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:21  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:22  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:22  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:22  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:22  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:22  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:22  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:22  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:23  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:26  ->  16 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:26  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:26  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:26  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:26  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:26  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:27  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:27  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:27  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:27  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:27  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:27  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:27  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:27  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:27  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:27  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F608550>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F81F6A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:29  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:29  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:29  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:29  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:29  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F7868E0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:29  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:29  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6ba6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:29  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:29  ->  Response: GET https://books.toscrape.com/catalogue/category/books/sports-and-games_17/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:29  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:30  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:30  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F786B20>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:30  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:30  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F81F7F0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:30  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F8D2970>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6aa"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:01 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cd3a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:30  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:30  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:30  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:30  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:30  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:30  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:30  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:30  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:31  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:31  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:31  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:31  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:31  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:31  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:31  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:34  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:34  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:05 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c6f3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:34  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:34  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:34  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:34  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:34  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:34  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:34  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:34  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:34  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:34  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:34  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:34  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:34  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:34  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:34  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:34  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:34  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:34  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:34  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:34  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:34  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:34  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:34  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:34  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:34  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:34  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:36  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:36  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:36  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:36  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:36  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:37  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:37  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:37  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:37  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:37  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:37  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:37  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:37  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:37  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:08 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c688"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:37  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:37  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:37  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F8DD5E0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:37  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:37  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:37  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:37  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:37  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:37  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:37  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:37  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:37  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:37  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:37  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:37  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:37  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:37  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:37  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197FA95310>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:37  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:38  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:09 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-cb93"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:38  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:38  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:38  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:38  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:38  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:38  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:38  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:38  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:38  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:38  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:38  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:38  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:38  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:38  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:38  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:38  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:38  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:38  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:38  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:39  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:39  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3c7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:40  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:40  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:40  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:40  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:40  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:40  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:41  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:41  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:41  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:41  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:41  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:42  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:42  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:42  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:42  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:42  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:43  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:43  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:43  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:44  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:44  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:44  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:44  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:44  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:44  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:44  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:44  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:44  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:44  ->  Getting page: https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:44  ->  Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:44  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c816"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:45  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:45  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:45  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197FA8B520>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:45  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:45  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:45  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:45  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:45  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:45  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:45  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:45  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:45  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:45  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:45  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:45  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:45  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:45  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:45  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197FB39CA0>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:45  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:45  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8a25"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:45  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:45  ->  Response: GET https://books.toscrape.com/catalogue/category/books/nonfiction_13/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:16 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c730"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:45  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:45  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:45  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:46  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:46  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:46  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:46  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:46  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:46  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:46  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:46  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:46  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:48  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:49  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:49  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:50  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:50  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:50  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:50  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:50  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:50  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:50  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:50  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:50  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:51  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:51  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:51  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:51  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:51  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:51  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:51  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:52  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197FBE9910>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:52  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:52  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F2D20D0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:23 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c830"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:52  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:52  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:52  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:52  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:52  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:52  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:52  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:52  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:52  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:52  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:52  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:52  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:52  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:52  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:52  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:52  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:52  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:24 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c753"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:53  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:53  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-5.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:53  ->  Getting page: https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:53  ->  Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:53  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:53  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:53  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:53  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:53  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:53  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:53  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:53  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:53  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:53  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:53  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:53  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:53  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:53  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:53  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197ED7DEB0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:53  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:53  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F634BB0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:53  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:53  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:54  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:25 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7659"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:54  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:54  ->  Response: GET https://books.toscrape.com/catalogue/category/books/add-a-comment_18/page-4.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:54  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:54  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:54  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:54  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:54  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:54  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:54  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:54  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:54  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:54  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:54  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:54  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:54  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:54  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:54  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:54  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:54  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:56  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:56  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:56  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:40:56  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:40:56  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:56  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:56  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:56  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:56  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:56  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:56  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:40:57  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:57  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:40:57  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:40:57  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:40:57  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:58  ->  Getting page: https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:58  ->  Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:58  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:58  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:29 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-703c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:58  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:58  ->  Response: GET https://books.toscrape.com/catalogue/category/books/new-adult_20/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:58  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:58  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:58  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:40:59  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:40:59  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:59  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:59  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:59  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:59  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:59  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:59  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:59  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:59  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:59  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:40:59  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:40:59  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:59  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197ED7DD60>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:59  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:40:59  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197ED4DE50>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:40:59  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:40:59  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:30 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c68f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:40:59  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:31 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c498"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:40:59  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:40:59  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-6.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:40:59  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:00  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:00  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:00  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:00  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:00  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:00  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:00  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:00  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:00  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:00  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:00  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:00  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:00  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:00  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:00  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:00  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:00  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:02  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:02  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:02  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:02  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:02  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:02  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:02  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:02  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:03  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:34 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c3f7"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:03  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:03  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:03  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:03  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:03  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:03  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:03  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:03  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:03  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:03  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:03  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:03  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:03  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:03  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:03  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:03  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:03  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:03  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:03  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:03  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:04  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:05  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:05  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:05  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:41:05  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:05  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:05  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:05  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:05  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:41:05  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:05  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:06  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:06  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:06  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:06  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:06  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:41:06  ->  going to next page
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:08  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:08  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:08  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:08  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:08  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:08  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F351F40>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:08  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:39 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7dd2"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:08  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:08  ->  Response: GET https://books.toscrape.com/catalogue/category/books/fantasy_19/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:08  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:08  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:08  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:08  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:08  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:08  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:08  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:08  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:08  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:08  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:08  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:08  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:08  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:08  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:08  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:09  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197ED4D0A0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:09  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:09  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c5ec"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-7.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:40 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c45c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:09  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:09  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:09  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:09  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:09  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:09  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:09  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:09  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:10  ->  8 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:10  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:10  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:10  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:10  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:10  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:10  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:11  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:12  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:13  ->  Getting page: https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:13  ->  Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:13  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:13  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:13  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:13  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:13  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:13  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:14  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:45 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-a633"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:14  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:14  ->  Response: GET https://books.toscrape.com/catalogue/category/books/science_22/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:14  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:14  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:14  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:14  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:41:14  ->  going to next page
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:14  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:14  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:14  ->  response_closed.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:14  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:14  ->  close.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:14  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:14  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:14  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:14  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:14  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:14  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:14  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:14  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:14  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:14  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:14  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:14  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:14  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:14  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:14  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:14  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:14  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:41:14  ->  going to next page
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:14  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:15  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:16  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:16  ->  14 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:16  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:16  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:16  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:16  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:16  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:16  ->  Getting page: https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:16  ->  Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:16  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:16  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:16  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:16  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:16  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:17  ->  Getting page: https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:17  ->  Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:17  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9417"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/default_15/page-8.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:17  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F351E80>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:17  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:17  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:17  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:17  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:17  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:17  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:17  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:17  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:17  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:17  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:17  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:17  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:17  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:17  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:17  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EC46640>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:48 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-9fbd"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:17  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:17  ->  Response: GET https://books.toscrape.com/catalogue/category/books/young-adult_21/page-3.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:17  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:17  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:18  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:18  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:18  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:18  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:18  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:18  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:18  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:18  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:18  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:18  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:18  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:18  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:18  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:18  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:18  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:18  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:18  ->  Getting page: https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:18  ->  Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:18  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:18  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:18  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:18  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:18  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:18  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:19  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:50 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-bc4f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:19  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:19  ->  Response: GET https://books.toscrape.com/catalogue/category/books/poetry_23/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:19  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:19  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:19  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:19  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:19  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:19  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:19  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:19  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:19  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:19  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:19  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:19  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:19  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:19  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:19  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:19  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:19  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:19  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:20  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:20  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:20  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:20  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:20  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:20  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:20  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:21  ->  14 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:21  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:21  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:21  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:21  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:21  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:21  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:22  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:23  ->  19 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:23  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:23  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:23  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:23  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:23  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:23  ->  Getting page: https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:23  ->  Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:23  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:23  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:23  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:23  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:23  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:23  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:23  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:55 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5389"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:24  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:24  ->  Response: GET https://books.toscrape.com/catalogue/category/books/paranormal_24/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:24  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:24  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:24  ->  Getting page: https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:24  ->  Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:24  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:24  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:24  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:24  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:24  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:55 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-7c19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:24  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:24  ->  Response: GET https://books.toscrape.com/catalogue/category/books/art_25/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:24  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:24  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:24  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:24  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:24  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:24  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:25  ->  Getting page: https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:25  ->  Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:56 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-78f8"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:25  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:25  ->  Response: GET https://books.toscrape.com/catalogue/category/books/psychology_26/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:25  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:25  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:25  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:25  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:25  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:25  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:25  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:25  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:25  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:25  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:25  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:25  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:25  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:25  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:25  ->  8 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:25  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:25  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:25  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:25  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:25  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:25  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:26  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:26  ->  7 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:26  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:26  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:26  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:26  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:26  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:27  ->  Getting page: https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:27  ->  Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:58 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-810a"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:27  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:27  ->  Response: GET https://books.toscrape.com/catalogue/category/books/autobiography_27/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:27  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:27  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:27  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:27  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:27  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:27  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:27  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:27  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:27  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:27  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:27  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:27  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:27  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:27  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:27  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:28  ->  Getting page: https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:28  ->  Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:41:59 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f4"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:28  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:28  ->  Response: GET https://books.toscrape.com/catalogue/category/books/parenting_28/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:28  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:28  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:28  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:28  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:28  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:28  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:28  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:28  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:28  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:28  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:28  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:28  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:28  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:28  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:28  ->  9 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:28  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:28  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:28  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:28  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:28  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:28  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:28  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:28  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:28  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:28  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:28  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:29  ->  Getting page: https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:29  ->  Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:00 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5381"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:29  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:29  ->  Response: GET https://books.toscrape.com/catalogue/category/books/adult-fiction_29/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:29  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:29  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:29  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:29  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:29  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:29  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:29  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:29  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:29  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:29  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:29  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:29  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:29  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:29  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:29  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:29  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:29  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:29  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:29  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:29  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:29  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:31  ->  Getting page: https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:31  ->  Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:31  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:02 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ad6"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:31  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:31  ->  Response: GET https://books.toscrape.com/catalogue/category/books/humor_30/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:31  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:31  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:31  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:31  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:31  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:31  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:31  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:31  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:31  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:31  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:31  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:31  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:31  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:31  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:31  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F0C6AF0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:31  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:31  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:32  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197EF97DF0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:32  ->  Getting page: https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:32  ->  Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:03 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-adde"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:32  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:32  ->  Response: GET https://books.toscrape.com/catalogue/category/books/horror_31/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:03 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-c096"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:32  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:32  ->  Response: GET https://books.toscrape.com/catalogue/category/books/history_32/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:32  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:32  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing save_to_db on dataset
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:32  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:32  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:32  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:32  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:32  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:32  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:33  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:34  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:34  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:34  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:34  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:34  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:34  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:34  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:35  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:36  ->  17 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:36  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:36  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:36  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:36  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:36  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:36  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:36  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:36  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:36  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:36  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:36  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:36  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:36  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:36  ->  18 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:36  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:36  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:36  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:36  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:36  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:36  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:07 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-d36c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:36  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:36  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:36  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:37  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:37  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:37  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:37  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:37  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:37  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:37  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:37  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:37  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:37  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:37  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:37  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:37  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:37  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:37  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:37  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:37  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:38  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:38  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:39  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:39  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:39  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:39  ->  Getting page: https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:39  ->  Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:39  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:10 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-705f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:39  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:39  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian-fiction_34/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:39  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:39  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:39  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:39  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:39  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:39  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:39  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:39  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:39  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:39  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:39  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:39  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:39  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:39  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197FC879A0>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:39  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:39  ->  20 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:39  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:39  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:39  ->  getting next page tag
[INFO|beautifulcrawler|WEB_SCRAPER|L337] - 2024-06-06 23:41:39  ->  going to next page
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:39  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197FC76E80>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:39  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:39  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:40  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:11 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-985e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:40  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:40  ->  Response: GET https://books.toscrape.com/catalogue/category/books/business_35/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:40  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:40  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:40  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:40  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:40  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:40  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:40  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:40  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:40  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:40  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:40  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:40  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:40  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:40  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:40  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:40  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:40  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:40  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:40  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:40  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:40  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:40  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:40  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:40  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:41  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:41  ->  12 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:41  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:41  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:41  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:41  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:41  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:13 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8e9e"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:42  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:42  ->  Response: GET https://books.toscrape.com/catalogue/category/books/food-and-drink_33/page-2.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:42  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:42  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:42  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:42  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:42  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:42  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:42  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:42  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:42  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:42  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:42  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:42  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:42  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:42  ->  Getting page: https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:42  ->  Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:42  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:42  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:43  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:14 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d54"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:43  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:43  ->  Response: GET https://books.toscrape.com/catalogue/category/books/biography_36/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:43  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:43  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:43  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:43  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:43  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:43  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:43  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:43  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:43  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:43  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:43  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:43  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:43  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:43  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:43  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:43  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:43  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:43  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:43  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:43  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:43  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:43  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:43  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:43  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:44  ->  Getting page: https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:44  ->  Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:44  ->  10 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:44  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:44  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:44  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:44  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:44  ->  Moving on!
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:15 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-8ce3"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:44  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:44  ->  Response: GET https://books.toscrape.com/catalogue/category/books/thriller_37/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:44  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:44  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:44  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:44  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:44  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:44  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:44  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:44  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:44  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:44  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:44  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:44  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:44  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:44  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:44  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:45  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:46  ->  11 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:46  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:46  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:46  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:46  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:46  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:46  ->  Getting page: https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:46  ->  Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:46  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:46  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:17 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5f19"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:46  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:46  ->  Response: GET https://books.toscrape.com/catalogue/category/books/contemporary_38/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:46  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:46  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:46  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:46  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:46  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:46  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:46  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:46  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:46  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:46  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:46  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:46  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:46  ->  Executing save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:46  ->  Getting page: https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:46  ->  Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:46  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:46  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:47  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:18 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-73d1"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:47  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:47  ->  Response: GET https://books.toscrape.com/catalogue/category/books/spirituality_39/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:47  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:47  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:47  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:47  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:47  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:47  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:47  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:47  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:47  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:47  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:47  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:47  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:47  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:47  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:47  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:47  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:47  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:47  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:47  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:47  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:47  ->  6 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:47  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:47  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:47  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:47  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:47  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:48  ->  Getting page: https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:48  ->  Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:48  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:48  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:48  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:48  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:48  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:20 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-536f"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:49  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:49  ->  Response: GET https://books.toscrape.com/catalogue/category/books/academic_40/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:49  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:49  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:49  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:49  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:49  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:49  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:49  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:49  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:49  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:49  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:49  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:49  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:49  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:49  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:49  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:49  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:49  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:49  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:49  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:49  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:49  ->  Getting page: https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:49  ->  Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:49  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6d5d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/self-help_41/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:50  ->  Getting page: https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:50  ->  Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:21 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-598c"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:50  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:50  ->  Response: GET https://books.toscrape.com/catalogue/category/books/historical_42/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:50  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:50  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:50  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:50  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:50  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:50  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:51  ->  2 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:51  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:51  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:51  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:51  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:51  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:51  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:51  ->  5 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:51  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:51  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:51  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:51  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:51  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:51  ->  Getting page: https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:51  ->  Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:22 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6056"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:51  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:51  ->  Response: GET https://books.toscrape.com/catalogue/category/books/christian_43/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:51  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:52  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:52  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:52  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:52  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:52  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:52  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:52  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:52  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:52  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:52  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:52  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:52  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:52  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:52  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:52  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:52  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:52  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:52  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:52  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:52  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:52  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:53  ->  Getting page: https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:53  ->  Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:53  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:53  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:53  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:53  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:53  ->  receive_response_headers.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:54  ->  Getting page: https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:54  ->  Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:54  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:25 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5372"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:54  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:54  ->  Response: GET https://books.toscrape.com/catalogue/category/books/suspense_44/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:54  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:54  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:54  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F402370>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:54  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:54  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:54  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:54  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:54  ->  Moving on!
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:41:54  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F402550>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:25 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5310"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:54  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:54  ->  Response: GET https://books.toscrape.com/catalogue/category/books/short-stories_45/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:54  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:54  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:54  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:54  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:54  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:54  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:54  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:54  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:54  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:55  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:55  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:55  ->  Getting page: https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:55  ->  Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:26 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53d0"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:55  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:55  ->  Response: GET https://books.toscrape.com/catalogue/category/books/novels_46/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:55  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:55  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:55  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:55  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:55  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:55  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:55  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:55  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:55  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:55  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:55  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:55  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:55  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:55  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:55  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:55  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:55  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:55  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:55  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:55  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:55  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:57  ->  Getting page: https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:57  ->  Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-6680"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/health_47/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:57  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:57  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:57  ->  Getting page: https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:57  ->  Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:57  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:28 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-608d"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:57  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:57  ->  Response: GET https://books.toscrape.com/catalogue/category/books/politics_48/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:57  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:57  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:57  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:57  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:57  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:58  ->  Executing save_to_db on dataset
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:58  ->  4 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:58  ->  Moving on!
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:58  ->  3 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:58  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:41:58  ->  Getting page: https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:41:58  ->  Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:29 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5315"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:41:58  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:41:58  ->  Response: GET https://books.toscrape.com/catalogue/category/books/cultural_49/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:41:58  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:41:58  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:41:58  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:58  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:58  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:58  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:58  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:58  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:58  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:58  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:58  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:58  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:58  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:41:58  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:41:58  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:41:58  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:41:58  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:41:58  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:41:58  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:41:58  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:41:58  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:41:58  ->  Worker-3 completed task in time:  154.97s
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:42:00  ->  Getting page: https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:42:00  ->  Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:00  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:00  ->  close.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:31 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-5300"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:42:00  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:42:00  ->  Response: GET https://books.toscrape.com/catalogue/category/books/erotica_50/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  receive_response_body.started request=<Request [b'GET']>
[INFO|beautifulcrawler|WEB_SCRAPER|L87] - 2024-06-06 23:42:00  ->  Getting page: https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|beautifulcrawler|WEB_SCRAPER|L72] - 2024-06-06 23:42:00  ->  Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:00  ->  connect_tcp.started host='books.toscrape.com' port=443 local_address=None timeout=5.0 socket_options=None
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:00  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:42:00  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:42:00  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:00  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:00  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:00  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:00  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:00  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:00  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:00  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:00  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:00  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:00  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:00  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:42:00  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:42:01  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:01  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:42:01  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:42:01  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:42:01  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:42:01  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:42:01  ->  Worker-1 completed task in time:  157.04s
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:01  ->  connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197E86F130>
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:01  ->  start_tls.started ssl_context=<ssl.SSLContext object at 0x000001197E82B0C0> server_hostname='books.toscrape.com' timeout=5.0
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:01  ->  start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x000001197F2D2CA0>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  send_request_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  send_request_headers.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  send_request_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  send_request_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  receive_response_headers.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 06 Jun 2024 22:42:32 GMT'), (b'Content-Type', b'text/html'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Wed, 08 Feb 2023 21:02:32 GMT'), (b'ETag', b'W/"63e40de8-53f5"'), (b'Strict-Transport-Security', b'max-age=0; includeSubDomains; preload'), (b'Content-Encoding', b'br')])
[INFO|_client|httpx|L1773] - 2024-06-06 23:42:01  ->  HTTP Request: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html "HTTP/1.1 200 OK"
[INFO|beautifulcrawler|WEB_SCRAPER|L77] - 2024-06-06 23:42:01  ->  Response: GET https://books.toscrape.com/catalogue/category/books/crime_51/index.html - Status 200
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  receive_response_body.started request=<Request [b'GET']>
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  receive_response_body.complete
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  response_closed.started
[DEBUG|_trace|httpcore.http11|L85] - 2024-06-06 23:42:01  ->  response_closed.complete
[INFO|beautifulcrawler|WEB_SCRAPER|L266] - 2024-06-06 23:42:01  ->  Parsing data
[INFO|beautifulcrawler|WEB_SCRAPER|L378] - 2024-06-06 23:42:01  ->  beginning pipeline
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:01  ->  Executing remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:01  ->  Finished execution of remove_nulls on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:01  ->  Executing clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:01  ->  Finished execution of clean_title on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:01  ->  Executing clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:01  ->  Finished execution of clean_rating on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:01  ->  Executing clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:01  ->  Finished execution of clean_price on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:01  ->  Executing clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:01  ->  Finished execution of clean_link on dataset
[DEBUG|beautifulcrawler|WEB_SCRAPER|L395] - 2024-06-06 23:42:01  ->  Executing save_to_db on dataset
[DEBUG|storage|WEB_SCRAPER|L195] - 2024-06-06 23:42:01  ->  INSERT INTO scraping.BookScrape 
                                        (category, title, rating, price_£, availability, link)
                                        VALUES (%s, %s, %s, %s, %s, %s)
[INFO|storage|WEB_SCRAPER|L209] - 2024-06-06 23:42:01  ->  1 records inserted into database!
[DEBUG|beautifulcrawler|WEB_SCRAPER|L397] - 2024-06-06 23:42:01  ->  Finished execution of save_to_db on dataset
[INFO|beautifulcrawler|WEB_SCRAPER|L407] - 2024-06-06 23:42:01  ->  Pipeline process complete
[DEBUG|beautifulcrawler|WEB_SCRAPER|L326] - 2024-06-06 23:42:01  ->  getting next page tag
[DEBUG|beautifulcrawler|WEB_SCRAPER|L249] - 2024-06-06 23:42:01  ->  Tag was not find
[DEBUG|beautifulcrawler|WEB_SCRAPER|L250] - 2024-06-06 23:42:01  ->  Moving on!
[INFO|beautifulcrawler|WEB_SCRAPER|L466] - 2024-06-06 23:42:01  ->  Worker-2 completed task in time:  157.92s
[INFO|beautifulcrawler|WEB_SCRAPER|L433] - 2024-06-06 23:42:01  ->  Worker(s) finished all work in time: 157.93s
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:01  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:01  ->  close.complete
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:01  ->  close.started
[DEBUG|_trace|httpcore.connection|L85] - 2024-06-06 23:42:01  ->  close.complete
